{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST449 - Assignment2-2021 - LT 2020-2021\n",
    "\n",
    "> **Important**:  Please note that you _must_ submit a solution for this coursework assessment. A failure to do this will result in a Zero Incomplete grade, and hence an overall fail of the course.\n",
    "\n",
    "## P1 (max points 25)\n",
    "\n",
    "Consider a two-player game that proceeds over rounds where in each round one player wins and other loses.  The winner of the game is the player who first wins  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d\" title=\"\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" /> rounds more than the opponent, where <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d{\\geq}1\" title=\"\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" /> is a parameter. \n",
    "\n",
    "Suppose we model this game so that we consider one of the players. In each round, this player has two available actions, either to invest _high_ or _low_ effort. \n",
    "\n",
    "If the player invests high effort in a round, she wins this round with probability  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H\" title=\"a\" /> , otherwise, she wins this round with probability <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L\" title=\"a\" /> .\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L\" title=\"a\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H\" title=\"a\" />  are parameters such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0<p_L<p_H<1\" title=\"x\" /> . \n",
    "\n",
    "By investing high effort in a round, the player incurs a cost of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />, and otherwise, a cost of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L\" title=\"x\" />, in this round.\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" /> are parameters such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0<c_L<c_H\" title=\"x\" />.\n",
    "\n",
    "The player receives a prize of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R\" title=\"x\" /> if winning the game. \n",
    "\n",
    "Assume the following values for parameters:\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;d=3\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H=0.55\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L=0.45\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H=50\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_L=10\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;{R=1000}\" title=\"x\" />\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. **[max points 5]** For each deterministic policy, compute the value function numerically by assuming perfect knowledge of the environment, and rank these policies with respect to the value at the initial state. What is the optimal deterministic policy? Discuss the results. \n",
    "2. **[max points 10]** Estimate the action-value function by using the off-policy Monte Carlo estimation method for a threshold policy <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\pi\" title=\"x\" />\n",
    "such that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\pi(s)=high\" title=\"x\" /> whenever <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;s{\\leq}s^*\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;{\\pi(s)=low}\" title=\"x\" />  otherwise, for given threshold value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;{s^*=1}\" title=\"x\" /> where the behaviour policy, in each state, selects either action equiprobably. Show action values. Can you conclude from the obtained results whether the target policy is optimal? Discuss your answer.\n",
    "3. **[max points 10]** Compute the optimal policy by using the off-policy Monte Carlo algorithm for the behavior policy that in each state selects either action equiprobably. Show action values and policy. \n",
    "\n",
    "In P1-1, use the termination condition <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;||V_{t+1}-V_t||_{\\infty}{\\leq}10^{-6}\" title=\"x\" />. \n",
    "\n",
    "In P1-2 and P2-3, use the number of episodes equal to 500,000. \n",
    "\n",
    "\n",
    "## P2 (max points 25)\n",
    "\n",
    "Consider the same reinforcement learning problem as in P1 but for the following questions:\n",
    "\n",
    "1. **[max points 5]** Solve the problem by using Q-learning algorithm. Use <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon\" title=\"x\" />-greedy with <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon=0.1\" title=\"x\" />. \n",
    "2. **[max points 5]** Solve the problem by using SARSA algorithm. Use <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon\" title=\"x\" />-greedy with  <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon=0.1\" title=\"x\" />.\n",
    "3. **[max points 15]** Assume that the agent follows a random policy, evaluate it by using on-line TD(<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda\" title=\"x\" />)-algorithm. Use the values of parameters <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda=0.9\" title=\"x\" /> and step size <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\eta=0.0001\" title=\"x\" />.\n",
    "\n",
    "In all questions, use the number of episodes equal to 500,000. For each question report the estimated values and policy. \n",
    "\n",
    "\n",
    "## P3 (max points 25)\n",
    "\n",
    "Consider a variant of the reinforcement learning problem which is identical to that in P1 but where the player does not incur a cost by investing effort but can invest high effort if she has sufficient energy. \n",
    "\n",
    "The player starts with given energy level <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;B\" title=\"x\" /> which is decremented by <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />whenever in a round the player invests high effort. \n",
    "If the energy level would become negative after subtracting the value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />, it is set equal to <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;0\" title=\"x\" />. \n",
    "In each round, an amount of energy of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;a\" title=\"x\" /> is added to the player, independently with probability <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p\" title=\"x\" />.  The maximum energy level is <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;B\" title=\"x\" />. \n",
    "\n",
    "The player can invest high effort in a round only if her energy level is at least <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H\" title=\"x\" />.\n",
    "\n",
    "The player receives a prize of value <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;R\" title=\"x\" /> if winning the game and this is the only reward received.\n",
    "\n",
    "Use the following setting of parameters:\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_H=0.55\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p_L=0.45\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;c_H=1\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;{R=1000}\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;B=10\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;a=2\" title=\"x\" />\n",
    "* <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;p=0.2\" title=\"x\" />\n",
    "* step size <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\eta=0.001\" title=\"x\" />\n",
    "* Discount parameter <img src=\"./figs/Discount.gif\" />\n",
    "\n",
    "\n",
    "1. Solve this problem by using SARSA (<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda\" title=\"x\" />) algorithm, for the value of parameter <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\lambda=0.9\" title=\"x\" />.  Assume that the policy followed by the agent is an <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon\" title=\"x\" />-greedy policy with <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\epsilon=0.1\" title=\"x\" />.\n",
    "Use 500,000 episodes.  Show the estimated action values for different states and policy. Discuss the results.\n",
    "\n",
    "\n",
    "## P4 (max points 25)\n",
    "\n",
    "Consider the following two-player game of chance. \n",
    "Two players, we refer to as players <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Y\" title=\"x\" />, have initial endowments of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" /> tokens, respectively. \n",
    "Assume that <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;{x,y>0}\" title=\"x\" /> . The game proceeds over rounds where in each round a dice is rolled. \n",
    "If the outcome of the dice is <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;1,2\" title=\"x\" /> or <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;3\" title=\"x\" />, player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;Y\" title=\"x\" /> loses one token, and otherwise, \n",
    "if the outcome is <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;4,5\" title=\"x\" /> or <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;6\" title=\"x\" />, player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> loses one token. \n",
    "The game ends as soon one of the players runs out of tokens. \n",
    "The winner of the game is the player who at the end of the game has at least one token left.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. **[max points 5]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x=1\" title=\"x\" /> and each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" />?\n",
    "\n",
    "2. **[max points 5]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y=1\" title=\"x\" /> and each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" />?\n",
    "\n",
    "3. **[max points 15]** What is the winning probability of player <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;X\" title=\"x\" /> for each value of <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;x\" title=\"x\" /> and <img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y\" title=\"x\" />?\n",
    "\n",
    "**Note:** You need to show derivations for your solutions.  \n",
    "\n",
    "## Marking scheme\n",
    "\n",
    "| **Problem breakdown** | **Max marks** |\n",
    "|-------------------|--------------:|\n",
    "| P1-1 Compute value function. Define optimal policy. Discuss results. | 5 |\n",
    "| P1-2 Estimate and show action values. Check if target policy is optimal. Discuss results. | 10 |\n",
    "| P1-3 Compute optimal policy. Show action values and policy. | 10 |\n",
    "| P2-1 Solution based on Q-learning. Report estimated values and policy. | 5 |\n",
    "| P2-2 Solution based on SARSA. Report estimated values and policy. | 5 |\n",
    "| P2-3 Evaluation using online TD. Report estimated values and policy. | 15 |\n",
    "| P3 Solution based on SARSA. Estimate action values for different states and policy. Discuss results. | 25 |\n",
    "| P4-1 Winning probabilities and values. Derivations of solution. | 5 |\n",
    "| P4-2 Winning probabilities and values. Derivations of solution. | 5 |\n",
    "| P4-3 Winning probabilities and values. Derivations of solution. | 15 |\n",
    "| Total | 100 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzqOCMxbvCgM"
   },
   "source": [
    "# P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Attributes.\n",
    "high = 0\n",
    "low = 1\n",
    "p_high = 0.55\n",
    "p_low = 0.45\n",
    "actions = [high, low]\n",
    "n_actions = len(actions)\n",
    "\n",
    "# Winning condition.\n",
    "win = 0\n",
    "lose = 1\n",
    "end_rounds = 3\n",
    "n_states = 2*end_rounds+1\n",
    "states = np.arange(0,n_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_end(status, end_rounds):\n",
    "    if (status == 0) or (status == 2 * end_rounds):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def effort_reward(action):\n",
    "    if action == high:\n",
    "        return -50\n",
    "    else:\n",
    "        return -10\n",
    "\n",
    "\n",
    "def EffortGame(states, end_rounds):\n",
    "    history = {}\n",
    "\n",
    "    for s in states:\n",
    "        history[s] = {a: [] for a in range(n_actions)}\n",
    "\n",
    "        for a in range(n_actions):\n",
    "            history[s][a] = {outcome: [] for outcome in range(n_actions)}\n",
    "\n",
    "            prob = p_high if a == high else p_low\n",
    "            reward = effort_reward(a)\n",
    "\n",
    "            if declare_end(s, end_rounds):\n",
    "                history[s][a][win] = [(1.0, s, 0.0, True)]\n",
    "                history[s][a][lose] = [(1.0, s, 0.0, True)]\n",
    "\n",
    "            else:\n",
    "                win_stat = s + 1\n",
    "                loss_stat = s - 1\n",
    "\n",
    "                if declare_end(win_stat, end_rounds):\n",
    "                    history[s][a][win] = [(prob, win_stat, 1000 + reward,\n",
    "                                           declare_end(win_stat, end_rounds))]\n",
    "                else:\n",
    "                    history[s][a][win] = [(prob, win_stat, reward,\n",
    "                                           declare_end(win_stat, end_rounds))]\n",
    "                history[s][a][lose] = [(prob, loss_stat, reward,\n",
    "                                        declare_end(loss_stat, end_rounds))]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63.98164815 162.18144099 316.42155381 560.97756912 803.53766247]\n"
     ]
    }
   ],
   "source": [
    "state_values_init = np.zeros(n_states)\n",
    "pi_a = np.zeros((n_states, n_actions))\n",
    "gamma = 1.0  # In Q, discount rate not specified. Hence zero discount.\n",
    "theta = 1e-6  # Termination condition.\n",
    "\n",
    "\n",
    "def policy(x):\n",
    "    pi_a = np.zeros((n_states, n_actions))\n",
    "    for i in range(len(x)):\n",
    "        pi_a[i + 1, x[i]] = 1.0\n",
    "    return pi_a\n",
    "\n",
    "\n",
    "def value_func_calc(pi_a, state_values_init, end_rounds):\n",
    "    state_values = state_values_init\n",
    "    transition_probs = EffortGame(states, end_rounds)\n",
    "    #iteration_counter = 1\n",
    "    while True:\n",
    "        v_old = np.copy(state_values)\n",
    "        delta = 0.0\n",
    "        for s in range(1, n_states - 1):\n",
    "            v_s = 0.0\n",
    "            for a in range(n_actions):\n",
    "                for outcome in range(n_actions):\n",
    "                    current_entry = transition_probs[s][a][outcome][0]\n",
    "                    p_sa = current_entry[0]\n",
    "                    next_s = current_entry[1]\n",
    "                    r = current_entry[2]\n",
    "                    v_s += pi_a[s, a] * p_sa * (r + gamma * v_old[next_s])\n",
    "\n",
    "            state_values[s] = v_s\n",
    "            delta = np.maximum(delta, np.abs(state_values[s] - v_old[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return state_values[1:2 * end_rounds]\n",
    "\n",
    "\n",
    "# Sense checking the value function calculation.\n",
    "example_policy = policy([low, low, low, high, high])\n",
    "values_history = value_func_calc(example_policy, state_values_init, end_rounds)\n",
    "print(values_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: LLLLL, Value: [ 30.34604732  87.43565984 183.95541753 341.35415514 594.60937021]\n",
      "Policy: LLLLH, Value: [ 42.97200715 115.49335002 233.67988178 423.79527785 728.08740233]\n",
      "Policy: LLLHL, Value: [ 41.94120882 113.20268685 229.62031616 417.06468375 628.67910733]\n",
      "Policy: LLLHH, Value: [ 63.9816482  162.18144045 316.42155393 560.97756829 803.53766256]\n",
      "Policy: LLHLL, Value: [ 28.25676978  82.79282103 175.72727633 336.71131633 592.52009267]\n",
      "Policy: LLHLH, Value: [ 48.55281489 127.895145   255.65861845 436.93870842 735.31628915]\n",
      "Policy: LLHHL, Value: [ 48.14369536 126.98598895 254.04739245 434.91835926 636.7132621 ]\n",
      "Policy: LLHHH, Value: [ 90.08730821 220.19401824 419.23273361 642.04731561 848.12602358]\n",
      "Policy: LHLLL, Value: [  4.06917839  29.04261773 148.73558022 321.48089229 585.66640184]\n",
      "Policy: LHLLH, Value: [ 23.03684547  71.19299087 206.40495612 407.48469111 719.11657969]\n",
      "Policy: LHLHL, Value: [ 19.84133809  64.09186316 196.6893207  392.99551764 617.84798261]\n",
      "Policy: LHLHH, Value: [ 54.27143199 140.6031822  301.37071878 549.10952619 797.01023941]\n",
      "Policy: LHHLL, Value: [ -8.44802635   1.22660728 110.67822082 300.00652005 576.0029343 ]\n",
      "Policy: LHHLH, Value: [ 25.31967319  76.2659413  213.34567463 411.63528692 721.39940741]\n",
      "Policy: LHHHL, Value: [ 19.35261054  63.00580034 195.20339128 391.90945482 617.35925506]\n",
      "Policy: LHHHH, Value: [100.25226135 242.78280388 441.17101843 659.34632236 857.64047677]\n",
      "Policy: HLLLL, Value: [-26.07753246  52.58630376 162.93598421 329.49365952 589.27214713]\n",
      "Policy: HLLLH, Value: [ -9.41253495  82.88630085 213.60431461 411.78995533 721.48447502]\n",
      "Policy: HLLHL, Value: [-11.78042839  78.58103857 206.40495956 400.09664773 621.04349188]\n",
      "Policy: HLLHH, Value: [ 17.39498298 131.62724178 295.11000023 544.17275873 794.2950173 ]\n",
      "Policy: HLHLL, Value: [-33.18164231  39.66974032 141.33661988 317.30593041 583.78766906]\n",
      "Policy: HLHLH, Value: [ -6.0795747   88.94622864 223.73786057 417.84988312 724.81743527]\n",
      "Policy: HLHHL, Value: [ -9.49760002  82.73163552 213.34568028 405.16959956 623.32632025]\n",
      "Policy: HLHHH, Value: [ 47.2922185  185.98585181 386.0096758  615.84992237 833.7174573 ]\n",
      "Policy: HHLLL, Value: [-74.66801366 -35.76002551 109.64978405 299.42621119 575.74179523]\n",
      "Policy: HHLLH, Value: [-48.61685989  11.6057093  169.71815099 385.54573736 707.05015555]\n",
      "Policy: HHLHL, Value: [-56.18453173  -2.15369498 152.26872374 360.52863487 603.23788607]\n",
      "Policy: HHLHH, Value: [ -8.45042045  84.63559919 262.33332959 518.32735546 780.0800455 ]\n",
      "Policy: HHHLL, Value: [-107.93384408  -96.24335382   32.9459271   256.14503785  556.26526732]\n",
      "Policy: HHHLH, Value: [-60.09806184  -9.26920242 143.24496652 369.7145977  698.34302836]\n",
      "Policy: HHHHL, Value: [-83.00739037 -50.92252892  90.42097534 315.32430041 582.89593558]\n",
      "Policy: HHHHH, Value: [ 38.30232872 169.64059866 370.13512339 603.33235493 826.83279467]\n"
     ]
    }
   ],
   "source": [
    "# Calculating all the possible combination of actions.\n",
    "action_list = list(itertools.product([0, 1], repeat=2 * end_rounds - 1))[::-1]\n",
    "\n",
    "dtype = [('policy', 'S10'), ('value_hist', float)]\n",
    "policy_hist = np.zeros(32, dtype=dtype)\n",
    "\n",
    "counter = 0\n",
    "for act in action_list:\n",
    "    _policy = \"\".join([\"H\" if x == high else \"L\" for x in act])\n",
    "    pi_a = policy(act)\n",
    "    values = value_func_calc(pi_a, state_values_init, end_rounds)\n",
    "    state = policy_hist[counter]\n",
    "    state['policy'] = _policy\n",
    "    state['value_hist'] = values[end_rounds - 1]\n",
    "    counter += 1\n",
    "    print(f\"Policy: {_policy}, Value: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'LHHHH', 441.17101843)\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(policy_hist, order=['value_hist'])[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the policy iteration result, playing (LHHHH) is the most optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold policy assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Attributes.\n",
    "high = 0\n",
    "low = 1\n",
    "win = 0\n",
    "lose = 1\n",
    "p_high = 0.55\n",
    "p_low = 0.45\n",
    "actions = [high, low]\n",
    "n_actions = len(actions)\n",
    "\n",
    "# Winning condition.\n",
    "end_rounds = 3\n",
    "n_states = 2*end_rounds+1\n",
    "states = np.arange(0,n_states)\n",
    "threshold = 1\n",
    "\n",
    "class EffortGame:\n",
    "    def __init__(self, init_state, end_rounds):\n",
    "        self.initial_state = init_state\n",
    "        self.state = self.initial_state\n",
    "        self.reward = 0.0\n",
    "        self.is_terminal = False\n",
    "        self.end_rounds = end_rounds\n",
    "\n",
    "    def effort_reward(self, action):\n",
    "        if action == high:\n",
    "            return -50\n",
    "        else:\n",
    "            return -10\n",
    "\n",
    "    def put_effort(self, action):\n",
    "        if action == high:\n",
    "            return np.random.binomial(1, p_high)\n",
    "        else:\n",
    "            return np.random.binomial(1, p_low)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = self.effort_reward(action)\n",
    "        is_won = self.put_effort(action)\n",
    "\n",
    "        if self.state == 2 * self.end_rounds - 1 and is_won:\n",
    "            self.state += 1\n",
    "            self.reward += 1000\n",
    "            self.is_terminal = True\n",
    "        elif self.state == 1 and not is_won:\n",
    "            self.state -= 1\n",
    "            self.is_terminal = True\n",
    "        else:\n",
    "            self.is_terminal = False\n",
    "            if is_won:\n",
    "                self.state += 1\n",
    "            else:\n",
    "                self.state -= 1\n",
    "        return self.state, self.reward, self.is_terminal\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        self.reward = 0.0\n",
    "        self.is_terminal = False\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Run the episodes.\n",
    "init_state = end_rounds\n",
    "env = EffortGame(init_state, end_rounds)\n",
    "episodes = 500000\n",
    "\n",
    "q_hist = np.zeros((n_states, n_actions))\n",
    "c_hist = np.zeros_like(q_hist)\n",
    "gamma = 1.0  # In Q, discount rate not specified. Hence zero discount.\n",
    "\n",
    "def behavior_policy():\n",
    "    return np.random.binomial(1, 0.5)\n",
    "\n",
    "\n",
    "# Thresholdhold policy. (After having discussion with the tutor.)\n",
    "def thresh_policy(state, action):\n",
    "    if state < thresh:\n",
    "        if action == high:\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.2\n",
    "    else:\n",
    "        if action == high:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "def assign_policy(policy_input):\n",
    "    pi_a = np.zeros((n_states, n_actions))\n",
    "    for i, pol in enumerate(policy_input):\n",
    "        pi_a[i + 1, pol] = 1\n",
    "    return pi_a\n",
    "\n",
    "# Sense check.\n",
    "target_policy = assign_policy([low, high, high, high, high])\n",
    "print(target_policy[1:-1])\n",
    "\n",
    "for i in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    history = []\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    while not done:\n",
    "        a = behavior_policy()\n",
    "        next_state, r, done = env.step(a)\n",
    "\n",
    "        history.append([t, state, a, r])\n",
    "        state = next_state\n",
    "        t += 1\n",
    "    # Reinitialise.\n",
    "    g = 0.0\n",
    "    w = 1.0\n",
    "\n",
    "    for t, state, action, reward in history[::-1]:\n",
    "        g = gamma * g + reward\n",
    "        c_hist[state, action] += w\n",
    "        q_hist[state,\n",
    "               action] += w * (g - q_hist[state, action]) / c_hist[state,\n",
    "                                                                   action]\n",
    "        w *= target_policy[state, action] / 0.5\n",
    "        if w == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 32.96645502  52.28686576]\n",
      " [161.61428689 157.70440165]\n",
      " [309.25210327 329.65734855]\n",
      " [502.76532235 509.40368018]\n",
      " [750.90831268 594.46058219]]\n",
      "LHLLH\n"
     ]
    }
   ],
   "source": [
    "print(q_hist[1:-1, :])\n",
    "optimal_policy = np.argmax(q_hist[1:-1, :], axis=1)\n",
    "print(\"\".join([\"H\" if x == high else \"L\" for x in optimal_policy]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the MC result, it is shown that the optimal policy is (LLHHL) which differs to the result in P1.1: (b'LHHHH', 441.17101843). Given the dynamic programming will always result the most optimal policy (due to the perfect information assumption), it can be concluded that the result of off-policy MC is not optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy:\n",
      "LHLLHLL\n",
      "[[ 42.26121396  34.42942831]\n",
      " [155.6938256  160.32604315]\n",
      " [332.27265242 345.5973071 ]\n",
      " [555.95710204 549.50045756]\n",
      " [789.75677545 778.76671512]]\n",
      "HLLHH\n"
     ]
    }
   ],
   "source": [
    "# Run the episodes.\n",
    "init_state = end_rounds\n",
    "env = EffortGame(init_state, end_rounds)\n",
    "episodes = 500000\n",
    "\n",
    "q_hist = np.zeros((n_states, n_actions))\n",
    "c_hist = np.zeros_like(q_hist)\n",
    "gamma = 1.0  # In Q, discount rate not specified. Hence zero discount.\n",
    "\n",
    "target_policy = np.zeros(n_states)\n",
    "\n",
    "\n",
    "def random_policy():\n",
    "    return np.random.binomial(1, 0.5)\n",
    "\n",
    "\n",
    "# Random policy assignment.\n",
    "for state in states:\n",
    "    target_policy[state] = random_policy()\n",
    "\n",
    "print(f\"Random policy:\")\n",
    "print(\"\".join([\"H\" if x == high else \"L\" for x in target_policy]))\n",
    "\n",
    "for i in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    history = []\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    while not done:\n",
    "        a = behavior_policy()\n",
    "        next_state, r, done = env.step(a)\n",
    "\n",
    "        history.append([t, state, a, r])\n",
    "        state = next_state\n",
    "        t += 1\n",
    "\n",
    "    g = 0.0\n",
    "    w = 1.0\n",
    "    for t, state, action, reward in history[::-1]:\n",
    "        g = gamma * g + reward\n",
    "        c_hist[state, action] += w\n",
    "        q_hist[state,\n",
    "               action] += w * (g - q_hist[state, action]) / c_hist[state,\n",
    "                                                                   action]\n",
    "        target_policy[state] = np.random.choice([\n",
    "            action_ for action_, value_ in enumerate(q_hist[state, :])\n",
    "            if value_ == np.max(q_hist[state, :])\n",
    "        ])\n",
    "        if target_policy[state] != action:\n",
    "            break\n",
    "        w *= 1.0 / 0.5\n",
    "\n",
    "print(q_hist[1:-1, :])\n",
    "print(\"\".join([\"H\" if x == high else \"L\" for x in target_policy[1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the MC result, it is shown that the optimal policy differs to the result in P1.1: (b'LHHHH', 441.17101843). Given the dynamic programming will always result the most optimal policy (due to the perfect information assumption), it can be concluded that the result of off-policy MC (Random policy) is not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2.1 & P2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning / SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon greedy policy.\n",
    "def eps_greedy_policy(qsa, epsilon=0.1):\n",
    "    if np.random.binomial(1, epsilon) == 1:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return np.random.choice([\n",
    "            action_ for action_, value_ in enumerate(qsa)\n",
    "            if value_ == np.max(qsa)\n",
    "        ])\n",
    "\n",
    "\n",
    "def q_learning(qsa, next_qs, r, alpha=0.1, gamma=1.0):\n",
    "    return qsa + alpha * (r + gamma * np.max(next_qs) - qsa)\n",
    "\n",
    "\n",
    "def sarsa(qsa, next_qsa, r, alpha=0.1, gamma=1.0):\n",
    "    return qsa + alpha * (r + gamma * next_qsa - qsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the episodes.\n",
    "init_state = end_rounds\n",
    "q_env = EffortGame(init_state, end_rounds)\n",
    "sarsa_env = EffortGame(init_state, end_rounds)\n",
    "episodes = 500000\n",
    "\n",
    "res_sarsa = np.zeros(episodes)\n",
    "res_qlearning = np.zeros_like(res_sarsa)\n",
    "\n",
    "# Leveraging the Seminar material.\n",
    "q_values = np.zeros((n_states, n_actions))\n",
    "sarsa_values = np.zeros_like(q_values)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state_q = q_env.reset()\n",
    "    state_sarsa = sarsa_env.reset()\n",
    "    done_q = False\n",
    "    done_sarsa = False\n",
    "    a_sarsa = eps_greedy_policy(sarsa_values[state_sarsa, :])\n",
    "    g_q = 0.0\n",
    "    g_sarsa = 0.0\n",
    "\n",
    "    while not done_q:\n",
    "        a_q = eps_greedy_policy(q_values[state_q, :])\n",
    "        next_state_q, r_q, done_q = q_env.step(a_q)\n",
    "        g_q += r_q\n",
    "        q_values[state_q, a_q] = q_learning(q_values[state_q, a_q],\n",
    "                                            q_values[next_state_q, :], r_q)\n",
    "        state_q = next_state_q\n",
    "\n",
    "    while not done_sarsa:\n",
    "        next_state_sarsa, r_sarsa, done_sarsa = sarsa_env.step(a_sarsa)\n",
    "        next_a = eps_greedy_policy(sarsa_values[next_state_sarsa, :])\n",
    "        g_sarsa += r_sarsa\n",
    "        sarsa_values[state_sarsa,\n",
    "                     a_sarsa] = sarsa(sarsa_values[state_sarsa, a_sarsa],\n",
    "                                      sarsa_values[next_state_sarsa,\n",
    "                                                   next_a], r_sarsa)\n",
    "        state_sarsa = next_state_sarsa\n",
    "        a_sarsa = next_a\n",
    "\n",
    "    res_sarsa[e] += g_sarsa\n",
    "    res_qlearning[e] += g_q\n",
    "\n",
    "q_optimal_policy = np.argmax(q_values, axis=1)\n",
    "sarsa_optimal_policy = np.argmax(sarsa_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_plot(result, title):\n",
    "    ax = plt.subplot()\n",
    "    plt.plot(result)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Sum of rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning Values\n",
      "[[0.00000000e+00 0.00000000e+00]\n",
      " [6.03482193e-01 3.86477385e+01]\n",
      " [4.34473150e+01 1.48497811e+02]\n",
      " [2.02022706e+02 3.71513429e+02]\n",
      " [5.26260900e+02 3.73916441e+02]\n",
      " [7.85568335e+02 6.89847010e+02]\n",
      " [0.00000000e+00 0.00000000e+00]]\n",
      "Optimal Policy:\n",
      "LLLHH\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Q-learning Values')\n",
    "print(q_values)\n",
    "print(f\"Optimal Policy:\")\n",
    "print(\"\".join([\"H\" if x == high else \"L\" for x in q_optimal_policy[1:-1]]))\n",
    "rewards_plot(res_sarsa, \"Q-Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA Values\n",
      "[[  0.           0.        ]\n",
      " [ -0.69382364  10.05055607]\n",
      " [ 44.91190045 116.04164674]\n",
      " [231.03817268 160.77144105]\n",
      " [440.4842828  354.8034749 ]\n",
      " [553.41068203 639.27441709]\n",
      " [  0.           0.        ]]\n",
      "Optimal Policy:\n",
      "LLHHL\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('SARSA Values')\n",
    "print(sarsa_values)\n",
    "print(f\"Optimal Policy:\")\n",
    "print(\"\".join([\"H\" if x == high else \"L\" for x in sarsa_optimal_policy[1:-1]]))\n",
    "rewards_plot(res_qlearning, \"SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2.3 TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = end_rounds\n",
    "td_env = EffortGame(init_state, end_rounds)\n",
    "episodes = 500000\n",
    "\n",
    "td_values = np.zeros(n_states)\n",
    "eligibility = np.zeros_like(td_values)\n",
    "\n",
    "# Hyperparameters.\n",
    "lam = 0.9  # In Q, Lambda is given.\n",
    "gamma = 1.0  # No discount.\n",
    "alpha = 0.0001  # In Q, step size eta = 0.0001.\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = td_env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = random_policy()\n",
    "        next_state, r, done = td_env.step(a)\n",
    "\n",
    "        delta = r + gamma * (td_values[next_state] - td_values[state])\n",
    "        eligibility[state] += 1\n",
    "\n",
    "        for s in range(1, 2 * end_rounds):\n",
    "            td_values[s] += alpha * delta * eligibility[s]\n",
    "            eligibility[s] = lam * gamma * eligibility[s]\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On-line TD(lambda) value function:\n",
      " [ 17.37535583  91.23237144 231.77075802 428.88416279 683.7261106 ]\n"
     ]
    }
   ],
   "source": [
    "print('On-line TD(lambda) value function:\\n', td_values[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Attributes.\n",
    "high = 0\n",
    "low = 1\n",
    "win = 0\n",
    "lose = 1\n",
    "p_high = 0.55\n",
    "p_low = 0.45\n",
    "actions = [high, low]\n",
    "n_actions = len(actions)\n",
    "full_energy = 10\n",
    "charge_energy = 2\n",
    "prob_charge = 0.2\n",
    "\n",
    "# Winning condition.\n",
    "end_rounds = 3\n",
    "n_states = 2 * end_rounds + 1\n",
    "states = np.arange(0, n_states)\n",
    "threshold = 1\n",
    "\n",
    "\n",
    "def put_effort(action):\n",
    "    if action == high:\n",
    "        return np.random.binomial(1, p_high)\n",
    "    else:\n",
    "        return np.random.binomial(1, p_low)\n",
    "\n",
    "\n",
    "def crank_battery(action):\n",
    "    if action == high:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class EnergyGame:\n",
    "    def __init__(self, init_state, full_energy, prob_charge, charge_energy):\n",
    "        self.initial_state = init_state\n",
    "        self.state = self.initial_state\n",
    "        self.reward = 0.0\n",
    "        self.is_terminal = False\n",
    "        self.full_energy = full_energy\n",
    "        self.current_energy = self.full_energy\n",
    "        self.prob_charge = prob_charge\n",
    "        self.charge_energy = charge_energy\n",
    "\n",
    "    def recharge_energy(self):\n",
    "        if np.random.binomial(1, self.prob_charge):\n",
    "            return min(self.full_energy,\n",
    "                       self.current_energy + self.charge_energy)\n",
    "        else:\n",
    "            return self.current_energy\n",
    "\n",
    "    def step(self, action):\n",
    "        is_won = put_effort(action)\n",
    "        if self.state == 2 * end_rounds - 1 and is_won:\n",
    "            self.state += 1\n",
    "            self.reward = 1000\n",
    "            self.is_terminal = True\n",
    "        elif self.state == 1 and not is_won:\n",
    "            self.state -= 1\n",
    "            self.reward = 0.0\n",
    "            self.is_terminal = True\n",
    "        else:\n",
    "            if is_won and action == high:\n",
    "                self.state += 1\n",
    "                self.is_terminal = False\n",
    "            elif is_won and action == low:\n",
    "                self.state += 1\n",
    "                self.is_terminal = False\n",
    "            elif not is_won and action == high:\n",
    "                self.state -= 1\n",
    "                self.is_terminal = False\n",
    "            else:\n",
    "                self.state -= 1\n",
    "                self.is_terminal = False\n",
    "            self.reward = 0.0\n",
    "\n",
    "        energy_consumption = crank_battery(action)\n",
    "\n",
    "        self.current_energy = max(self.current_energy - energy_consumption, 0)\n",
    "        self.current_energy = self.recharge_energy()\n",
    "\n",
    "        return self.state, self.current_energy, self.reward, self.is_terminal\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        self.reward = 0.0\n",
    "        self.is_terminal = False\n",
    "        self.current_energy = self.full_energy\n",
    "        return self.state, self.current_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = end_rounds\n",
    "episodes = 500000\n",
    "\n",
    "q_hist = np.zeros((n_states, full_energy+1, n_actions))\n",
    "eligibility = np.zeros_like(q_hist)\n",
    "\n",
    "# Hyperparameters.\n",
    "lam = 0.9 # In Q, Lambda is given.\n",
    "gamma = 1.0 # No discount.\n",
    "alpha = 0.0001 # In Q, step size eta = 0.0001.\n",
    "\n",
    "env = EnergyGame(init_state, full_energy, prob_charge, charge_energy)\n",
    "\n",
    "# Rewrite Epsilon Greedy Policy for P3.\n",
    "def eps_greedy_policy(qsa, energy_level, epsilon=0.1):\n",
    "    if energy_level < 1:\n",
    "        return low\n",
    "    else:\n",
    "        if np.random.binomial(1, epsilon) == 1:\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            return np.random.choice([action_ for action_, value_ in enumerate(qsa) if value_ == np.max(qsa)])\n",
    "        \n",
    "for e in range(episodes):\n",
    "    state, cur_energy = env.reset()\n",
    "    done = False\n",
    "    action = eps_greedy_policy(q_hist[state, cur_energy, :], cur_energy)\n",
    "\n",
    "    while not done:\n",
    "        next_state, next_energy, r, done = env.step(action)\n",
    "        n_action = eps_greedy_policy(q_hist[next_state, next_energy, :],\n",
    "                                   next_energy)\n",
    "        delta = r + gamma * q_hist[next_state, cur_energy,\n",
    "                                     next_a] - q_hist[state, cur_energy, action]\n",
    "        eligibility[state, cur_energy, action] += 1\n",
    "\n",
    "        for s in range(1, 2 * end_rounds):\n",
    "            for en in range(full_energy + 1):\n",
    "                for a in range(n_actions):\n",
    "                    q_hist[s, en, a] += alpha * delta * eligibility[s, en, a]\n",
    "                    eligibility[s, en, a] = gamma * lam * eligibility[s, en, a]\n",
    "\n",
    "        state = next_state\n",
    "        cur_energy = next_energy\n",
    "        action = n_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Value function history:\n",
      " [[[ 0.00000000e+00  8.34026204e-01]\n",
      "  [ 3.63483589e+00  2.67126878e-01]\n",
      "  [ 1.00758266e+01  8.88030492e-01]\n",
      "  [ 5.84433948e-01  3.05972691e+00]\n",
      "  [ 1.44374775e+01 -1.08600026e-01]\n",
      "  [ 1.30185292e+01  1.35150726e-01]\n",
      "  [ 4.92652461e+00  2.88722104e+00]\n",
      "  [ 8.79024855e+00  1.06134741e+00]\n",
      "  [ 1.06030577e+01  8.29359082e-02]\n",
      "  [ 1.75718421e+01  1.07788350e+01]\n",
      "  [ 8.40046407e+01  1.60746195e+01]]\n",
      "\n",
      " [[ 0.00000000e+00  7.60295511e+00]\n",
      "  [ 3.60859218e+00  2.88108560e+01]\n",
      "  [ 1.65982482e+00  1.95221323e+01]\n",
      "  [ 3.47412412e+01  7.58855777e+00]\n",
      "  [ 1.18698525e+01  4.33991820e+01]\n",
      "  [ 1.98044616e+01 -2.92365814e-01]\n",
      "  [ 8.50031695e+00  5.48342284e-01]\n",
      "  [ 2.57870116e+01  1.71592753e+01]\n",
      "  [ 1.77330213e+01  5.10038667e+01]\n",
      "  [ 3.14122729e+01  5.18997385e+01]\n",
      "  [ 1.75402307e+02  1.01999471e+02]]\n",
      "\n",
      " [[ 0.00000000e+00  3.82892710e+01]\n",
      "  [ 4.77345607e+01  4.14450351e+00]\n",
      "  [ 7.95185576e+01  9.28606818e+00]\n",
      "  [ 7.09036757e+01  2.72904142e+00]\n",
      "  [ 7.20648226e+01  5.48711590e+00]\n",
      "  [ 2.78941207e+01  1.66338111e+01]\n",
      "  [ 2.28250773e+01  4.82581335e+01]\n",
      "  [ 3.60214681e+01  1.13137371e+02]\n",
      "  [ 9.53852073e+01  3.58288452e+01]\n",
      "  [ 9.88771419e+01  7.38415095e+01]\n",
      "  [ 2.47455579e+02  2.84430056e+02]]\n",
      "\n",
      " [[ 0.00000000e+00  1.10249730e+02]\n",
      "  [ 1.54492711e+01  1.52328259e+02]\n",
      "  [ 1.13389171e+02  6.03018201e+00]\n",
      "  [ 1.78040254e+02  2.26980614e+01]\n",
      "  [ 3.99755534e+01  1.63886458e+02]\n",
      "  [ 8.71904662e+00  9.81622161e+01]\n",
      "  [ 9.12753365e+01  3.02842187e+01]\n",
      "  [ 2.25574520e+02  3.23150765e+02]\n",
      "  [ 1.55576902e+02  3.37718337e+02]\n",
      "  [ 1.36775763e+02  1.06262743e+02]\n",
      "  [ 4.08261898e+02  4.71589486e+02]]\n",
      "\n",
      " [[ 0.00000000e+00  1.38851883e+02]\n",
      "  [ 1.72182297e+01  2.06652320e+02]\n",
      "  [ 3.03343788e+02  2.19634689e+01]\n",
      "  [ 6.31768796e+00  1.09584848e+02]\n",
      "  [ 2.78561794e+01  2.93570647e+02]\n",
      "  [ 4.70285341e+02  4.70501052e+01]\n",
      "  [ 2.01014466e+01  2.21182630e+02]\n",
      "  [ 6.34604547e+02  1.19579996e+02]\n",
      "  [ 6.24550367e+02  1.98091164e+02]\n",
      "  [ 1.42914793e+02  5.21362650e+02]\n",
      "  [ 7.32090420e+02  3.95993613e+02]]]\n",
      "Optimal Policy:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = np.zeros((n_states, full_energy + 1))\n",
    "\n",
    "for s in range(0, 2 * end_rounds + 1):\n",
    "    for eng in range(0, full_energy + 1):\n",
    "        optimal_policy[s, eng] = np.argmax(q_hist[s, eng, :])\n",
    "\n",
    "print('Q-Value function history:\\n', q_hist[1:2 * end_rounds])\n",
    "print(f\"Optimal Policy:\")\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC based probability calculation.\n",
    "def dice_roll():\n",
    "    return np.random.randint(1, 7)\n",
    "\n",
    "\n",
    "def judgement(x, y, dice):\n",
    "    if dice <= 3:\n",
    "        y -= 1\n",
    "    else:\n",
    "        x -= 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def play_game(simulations, x, y):\n",
    "    x_win = []\n",
    "    y_win = []\n",
    "    for i in range(simulations):\n",
    "        x_tok, y_tok = x, y\n",
    "        while True:\n",
    "            dice = dice_roll()\n",
    "            x_tok, y_tok = judgement(x_tok, y_tok, dice)\n",
    "\n",
    "            if x_tok == 0:\n",
    "                y_win.append(1)\n",
    "                break\n",
    "            elif y_tok == 0:\n",
    "                x_win.append(1)\n",
    "                break\n",
    "    return x_win, y_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 1\n",
      "Probability of X winning: 49.997%\n",
      "Probability of Y winning: 50.003%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 2\n",
      "Probability of X winning: 24.8864%\n",
      "Probability of Y winning: 75.1136%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 3\n",
      "Probability of X winning: 12.5504%\n",
      "Probability of Y winning: 87.4496%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 4\n",
      "Probability of X winning: 6.2826%\n",
      "Probability of Y winning: 93.7174%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 5\n",
      "Probability of X winning: 3.1242%\n",
      "Probability of Y winning: 96.8758%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 6\n",
      "Probability of X winning: 1.577%\n",
      "Probability of Y winning: 98.423%\n"
     ]
    }
   ],
   "source": [
    "simulation = 500000\n",
    "x_token = 1\n",
    "y_token = [1,2,3,4,5,6]\n",
    "\n",
    "for y in y_token:\n",
    "    x_win, y_win = play_game(simulation, x_token, y)\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"# X token: {x_token} / # Y token: {y}\")\n",
    "    print(f\"Probability of X winning: {100*len(x_win)/simulation}%\")\n",
    "    print(f\"Probability of Y winning: {100*len(y_win)/simulation}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 1\n",
      "Probability of X winning: 49.8984%\n",
      "Probability of Y winning: 50.1016%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 1\n",
      "Probability of X winning: 74.9442%\n",
      "Probability of Y winning: 25.0558%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 1\n",
      "Probability of X winning: 87.4954%\n",
      "Probability of Y winning: 12.5046%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 1\n",
      "Probability of X winning: 93.7438%\n",
      "Probability of Y winning: 6.2562%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 1\n",
      "Probability of X winning: 96.9144%\n",
      "Probability of Y winning: 3.0856%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 1\n",
      "Probability of X winning: 98.4546%\n",
      "Probability of Y winning: 1.5454%\n"
     ]
    }
   ],
   "source": [
    "# P2\n",
    "simulation = 500000\n",
    "y_token = 1\n",
    "x_token = [1,2,3,4,5,6]\n",
    "\n",
    "for x in x_token:\n",
    "    x_win, y_win = play_game(simulation, x, y_token)\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"# X token: {x} / # Y token: {y_token}\")\n",
    "    print(f\"Probability of X winning: {100*len(x_win)/simulation}%\")\n",
    "    print(f\"Probability of Y winning: {100*len(y_win)/simulation}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 4\n",
      "Probability of X winning: 34.3516%\n",
      "Probability of Y winning: 65.6484%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 3\n",
      "Probability of X winning: 65.5804%\n",
      "Probability of Y winning: 34.4196%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 1\n",
      "Probability of X winning: 87.5314%\n",
      "Probability of Y winning: 12.4686%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 4\n",
      "Probability of X winning: 63.5944%\n",
      "Probability of Y winning: 36.4056%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 6\n",
      "Probability of X winning: 25.342%\n",
      "Probability of Y winning: 74.658%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 1\n",
      "Probability of X winning: 96.8782%\n",
      "Probability of Y winning: 3.1218%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 2\n",
      "Probability of X winning: 49.959%\n",
      "Probability of Y winning: 50.041%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 6\n",
      "Probability of X winning: 1.5686%\n",
      "Probability of Y winning: 98.4314%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 5\n",
      "Probability of X winning: 10.9342%\n",
      "Probability of Y winning: 89.0658%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 3\n",
      "Probability of X winning: 12.4406%\n",
      "Probability of Y winning: 87.5594%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 2\n",
      "Probability of X winning: 93.74%\n",
      "Probability of Y winning: 6.26%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 5\n",
      "Probability of X winning: 62.2422%\n",
      "Probability of Y winning: 37.7578%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 2\n",
      "Probability of X winning: 81.219%\n",
      "Probability of Y winning: 18.781%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 5\n",
      "Probability of X winning: 36.2918%\n",
      "Probability of Y winning: 63.7082%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 3\n",
      "Probability of X winning: 50.0238%\n",
      "Probability of Y winning: 49.9762%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 6\n",
      "Probability of X winning: 37.819%\n",
      "Probability of Y winning: 62.181%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 6\n",
      "Probability of X winning: 14.5288%\n",
      "Probability of Y winning: 85.4712%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 3\n",
      "Probability of X winning: 77.325%\n",
      "Probability of Y winning: 22.675%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 4\n",
      "Probability of X winning: 18.7916%\n",
      "Probability of Y winning: 81.2084%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 2\n",
      "Probability of X winning: 24.95%\n",
      "Probability of Y winning: 75.05%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 1\n",
      "Probability of X winning: 74.9804%\n",
      "Probability of Y winning: 25.0196%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 5\n",
      "Probability of X winning: 3.1426%\n",
      "Probability of Y winning: 96.8574%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 1\n",
      "Probability of X winning: 98.419%\n",
      "Probability of Y winning: 1.581%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 4\n",
      "Probability of X winning: 74.5444%\n",
      "Probability of Y winning: 25.4556%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 2\n",
      "Probability of X winning: 68.7976%\n",
      "Probability of Y winning: 31.2024%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 1\n",
      "Probability of X winning: 93.8002%\n",
      "Probability of Y winning: 6.1998%\n",
      "--------------------------------------\n",
      "# X token: 3 / # Y token: 5\n",
      "Probability of X winning: 22.7756%\n",
      "Probability of Y winning: 77.2244%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 2\n",
      "Probability of X winning: 89.0366%\n",
      "Probability of Y winning: 10.9634%\n",
      "--------------------------------------\n",
      "# X token: 4 / # Y token: 4\n",
      "Probability of X winning: 50.079%\n",
      "Probability of Y winning: 49.921%\n",
      "--------------------------------------\n",
      "# X token: 5 / # Y token: 5\n",
      "Probability of X winning: 49.9644%\n",
      "Probability of Y winning: 50.0356%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 1\n",
      "Probability of X winning: 50.0348%\n",
      "Probability of Y winning: 49.9652%\n",
      "--------------------------------------\n",
      "# X token: 1 / # Y token: 4\n",
      "Probability of X winning: 6.2642%\n",
      "Probability of Y winning: 93.7358%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 3\n",
      "Probability of X winning: 31.2312%\n",
      "Probability of Y winning: 68.7688%\n",
      "--------------------------------------\n",
      "# X token: 2 / # Y token: 6\n",
      "Probability of X winning: 6.2656%\n",
      "Probability of Y winning: 93.7344%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 6\n",
      "Probability of X winning: 50.1024%\n",
      "Probability of Y winning: 49.8976%\n",
      "--------------------------------------\n",
      "# X token: 6 / # Y token: 3\n",
      "Probability of X winning: 85.5362%\n",
      "Probability of Y winning: 14.4638%\n"
     ]
    }
   ],
   "source": [
    "# P3\n",
    "simulation = 500000\n",
    "y_token = [1,2,3,4,5,6]\n",
    "x_token = [1,2,3,4,5,6]\n",
    "prod = list(itertools.product(y_token, x_token))\n",
    "uniq_prod = list(set(prod))\n",
    "\n",
    "for x, y in uniq_prod:\n",
    "    x_win, y_win = play_game(simulation, x, y)\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"# X token: {x} / # Y token: {y}\")\n",
    "    print(f\"Probability of X winning: {100*len(x_win)/simulation}%\")\n",
    "    print(f\"Probability of Y winning: {100*len(y_win)/simulation}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ST449 Assignment 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
