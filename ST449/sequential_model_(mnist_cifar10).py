# -*- coding: utf-8 -*-
"""Sequential Model (MNIST / CIFAR10)의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Qhd4jvYLAjwbUCTnOSQ3n6qj6ytGXq2
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import IPython.display as display
import tensorflow_datasets as tfds

"""# P1

## Network Construction

### Hyperparameter
"""

EPOCHS = 45
BATCH_SIZE = 128
LR_RATE = 0.001
MOMENTUM = 0.7
L2_LAMBDA = 0.01

""" ### Data"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Preprocess the data.
x_train = x_train.reshape(60000, 784).astype("float32") / 255
x_test = x_test.reshape(10000, 784).astype("float32") / 255

y_train = y_train.astype("float32")
y_test = y_test.astype("float32")

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

"""### Loss Function

#### L2 Regularisation
"""

class RegularisedLayer(layers.Layer):
    def call(self, inputs):
        self.add_loss(tf.reduce_sum(inputs**2) * L2_LAMBDA)
        return inputs  # Pass-through layer.

"""#### Cross Entropy"""

class CrossEntropy(keras.losses.Loss):
    def __init__(self, name="loss"):
        super().__init__(name=name)

    def call(self, y_true, y_pred):
      # y_true_one_hot = tf.one_hot(y_true, depth=10)

      # Replace prediction values to avoid log(0) error.
      y_pred = tf.clip_by_value(y_pred, 1e-15, 1.)
      # Cross entropy.
      entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), 1))
      return entropy

"""### Learning Rate Decay Schedule"""

# Learning rate decay function.
def lr_decay(epoch):
  return LR_RATE / math.sqrt(1+epoch)

# Utility callback that reflects the learning rate decay.
lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)

def plot_learning_rate(lr_func, epochs):
  xx = np.arange(epochs+1, dtype=np.float)
  y = [lr_decay(x) for x in xx]
  fig, ax = plt.subplots(figsize=(9, 6))
  ax.set_xlabel('epochs')
  ax.set_title('Learning rate\ndecays from {:0.3g} to {:0.3g}'.format(y[0], y[-2]))
  ax.minorticks_on()
  ax.grid(True, which='major', axis='both', linestyle='-', linewidth=1)
  ax.grid(True, which='minor', axis='both', linestyle=':', linewidth=0.5)
  ax.step(xx,y, linewidth=3, where='post')

# Display the learning rate decay.
plot_learning_rate(lr_decay, EPOCHS)

"""### Visualisation functions"""

def accuracy_result(model):
  # Result for accuracy.
  plt.plot(model.history['accuracy'])
  plt.plot(model.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

def loss_result(model):
  # Result for loss.
  plt.plot(model.history['loss'])
  plt.plot(model.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

"""### Network Architecture"""

class LogitNetwork(tf.keras.Model):

  def __init__(self):
    super(LogitNetwork, self).__init__()

    # Initialising the network layer.
    self.initialiser = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)

  def initialise(self):
    # Network design.
    model = tf.keras.Sequential() # Initial network.
    model.add(RegularisedLayer())
    model.add(tf.keras.layers.Dense(10, 
                                activation='softmax', 
                                kernel_initializer=self.initialiser,
                                #kernel_regularizer=tf.keras.regularizers.L2(0.01)
                                )) # Output layer.
    return model
    

  def __call__(self, optimiser):
    model = self.initialise()
    model.compile(optimizer=optimiser, 
                  loss=CrossEntropy(),
                  metrics=["accuracy"]
                  )
    
    return model

"""## Network initialisation"""

model = LogitNetwork()

"""## Testing different optimisers"""

# Dictionary of 4 different optimisers.
optimiser_list = {
    "SGD": tf.optimizers.SGD(learning_rate=LR_RATE, momentum=MOMENTUM, name="SGD"),
    "Nesterov": tf.keras.optimizers.SGD(learning_rate=LR_RATE, momentum=MOMENTUM, nesterov=True, name='Nesterov'),
    "AdaGrad": tf.keras.optimizers.Adagrad(learning_rate=LR_RATE, name='AdaGrad'),
    "ADAM": tf.keras.optimizers.Adam(learning_rate=LR_RATE, name='ADAM')
}

# Looping through 4 different optimisers.
def loop_model(optimiser_list, x_train, y_train, BATCH_SIZE, EPOCHS):

  # Initialise output dictionary.
  model_config = {}
  model_history = {}

  # Implement optimiser and fitting the network.
  for key, item in optimiser_list.items():
    print("\n")
    print(f"#### {key} Training ####")
    model_config[key] = model(item)
    model_history[key] = model_config[key].fit(x_train,
                                               y_train,
                                               batch_size=BATCH_SIZE,
                                               epochs=EPOCHS,
                                               validation_split=0.1,
                                               verbose=True,
                                               callbacks=[lr_decay_callback])
  return model_config, model_history

model_config, model_history = loop_model(optimiser_list, x_train, y_train, BATCH_SIZE, EPOCHS)

"""### SGD"""

accuracy_result(model_history["SGD"])

loss_result(model_history["SGD"])

# Model evaluation.
print("Evaluate on test data")
results = model_config["SGD"].evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print("test loss, test acc:", results)

"""### Nesterov Momentum SGD"""

accuracy_result(model_history["Nesterov"])

loss_result(model_history["Nesterov"])

# Model evaluation.
print("Evaluate on test data")
results = model_config["Nesterov"].evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print("test loss, test acc:", results)

"""### AdaGrad"""

accuracy_result(model_history["AdaGrad"])

loss_result(model_history["AdaGrad"])

# Model evaluation.
print("Evaluate on test data")
results = model_config["AdaGrad"].evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print("test loss, test acc:", results)

"""### Adam"""

accuracy_result(model_history["ADAM"])

loss_result(model_history["ADAM"])

# Model evaluation.
print("Evaluate on test data")
results = model_config["ADAM"].evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print("test loss, test acc:", results)

"""# P2

## Network Construction

### Hyperparameter
"""

EPOCHS = 100
BATCH_SIZE = 128
LR_RATE = 0.001
STEPS_PER_EPOCH = 50

"""### Data"""

from keras.datasets import cifar10
(x_train,y_train), (x_test,y_test) = cifar10.load_data()

# Convert from integers to floats + Normalisation to range 0 to 1.
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

np.where(y_train[1] == 1)[0][0]

# setting class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    # The CIFAR labels happen to be arrays, 
    # which is why you need the extra index
    plt.xlabel(class_names[np.where(y_train[i] == 1)[0][0]])
plt.show()

"""## Network Architecture"""

class CnnNetwork(tf.keras.Model):

  def __init__(self):
    super(CnnNetwork, self).__init__()

  def conv_layer(self, model, filters):
    model.add(layers.Conv2D(filters, (5,5), padding='same', strides=1, kernel_initializer='he_uniform', use_bias=False))
    model.add(layers.BatchNormalization(center=True, scale=False))
    model.add(tf.keras.layers.Activation('relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
    model.add(layers.Dropout(0.4))
    return model

  def initialise(self):
    # Network design.
    model = tf.keras.Sequential()
    model.add(layers.Input(shape=(32, 32, 3), dtype=tf.float32))

    # Convolution 1.
    model = self.conv_layer(model, 64)
    
    # Convolution 2.
    model = self.conv_layer(model, 128)

    # Convolution 3.
    model = self.conv_layer(model, 256)

    # Full Connected Layer.
    model.add(layers.Flatten())
    model.add(layers.Dense(1000, activation='relu', kernel_initializer='he_uniform'))
    model.add(layers.Dropout(0.4))
    model.add(layers.Dense(10, activation='softmax'))
  
    return model
    

  def __call__(self, optimiser):
    model = self.initialise()
    model.compile(optimizer=optimiser, 
                  loss="categorical_crossentropy",
                  metrics=["accuracy"]
                  )
    
    return model

cnn_classifer = CnnNetwork()

sgd = tf.keras.optimizers.SGD(learning_rate=LR_RATE, momentum=MOMENTUM, name="SGD")

cnn_classifer = cnn_classifer(sgd)

cnn_classifer.summary()

cnn_history = cnn_classifer.fit(x_train, 
                                y_train, 
                                epochs=EPOCHS,
                                shuffle=True,
                                batch_size=BATCH_SIZE,
                                steps_per_epoch=STEPS_PER_EPOCH,
                                validation_split=0.1)
_, acc = cnn_classifer.evaluate(x_test, y_test, verbose=0)
print("Model Evaluation result: > %.3f" % (acc * 100.0))

"""### Visualisation functions"""

def accuracy_result(model):
  # Result for accuracy.
  plt.plot(model.history['accuracy'])
  plt.plot(model.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

def loss_result(model):
  # Result for loss.
  plt.plot(model.history['loss'])
  plt.plot(model.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

accuracy_result(cnn_history)

loss_result(cnn_history)

class CnnNetwork(tf.keras.Model):

  def __init__(self):
    super(CnnNetwork, self).__init__()

  def conv_layer(self, model, filters):
    model.add(layers.Conv2D(filters, (5,5), padding='same', strides=1, kernel_initializer='he_uniform', use_bias=False))
    # model.add(layers.BatchNormalization(center=True, scale=False))
    model.add(tf.keras.layers.Activation('relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
    # model.add(layers.Dropout(0.4))
    return model

  def initialise(self):
    # Network design.
    model = tf.keras.Sequential()
    model.add(layers.Input(shape=(32, 32, 3), dtype=tf.float32))

    # Convolution 1.
    model = self.conv_layer(model, 64)
    
    # Convolution 2.
    model = self.conv_layer(model, 128)

    # Convolution 3.
    model = self.conv_layer(model, 256)

    # Full Connected Layer.
    model.add(layers.Flatten())
    model.add(layers.Dense(1000, activation='relu', kernel_initializer='he_uniform'))
    # model.add(layers.Dropout(0.4))
    model.add(layers.Dense(10, activation='softmax'))
  
    return model
    

  def __call__(self, optimiser):
    model = self.initialise()
    model.compile(optimizer=optimiser, 
                  loss="categorical_crossentropy",
                  metrics=["accuracy"]
                  )
    
    return model